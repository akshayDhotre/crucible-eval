{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepEval Usage Demo (End-to-End)\n",
    "\n",
    "This notebook shows how to use a Crucible-generated DeepEval export and run an evaluation loop.\n",
    "\n",
    "## What this covers\n",
    "1. Load exported `deepeval` config JSON from Crucible.\n",
    "2. Generate actual outputs by calling your app endpoint.\n",
    "3. Build `LLMTestCase` objects.\n",
    "4. Run DeepEval metrics and inspect results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, uncomment and run once:\n",
    "# %pip install -U deepeval requests pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric, HallucinationMetric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Demo: Load Crucible export\n",
    "\n",
    "Set path to the downloaded `.json` file from Crucible (output format: `deepeval`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR if (NOTEBOOK_DIR / 'backend').exists() else NOTEBOOK_DIR.parent\n",
    "DOWNLOADS_DIR = PROJECT_ROOT / 'downloads'\n",
    "\n",
    "# Option 1: set explicit filename\n",
    "# DEEPEVAL_EXPORT_PATH = DOWNLOADS_DIR / 'crucible_rag_deepeval_YYYYMMDD_HHMMSS.json'\n",
    "\n",
    "# Option 2: auto-pick latest deepeval export\n",
    "candidates = sorted(DOWNLOADS_DIR.glob('crucible_*_deepeval_*.json'))\n",
    "DEEPEVAL_EXPORT_PATH = candidates[-1] if candidates else DOWNLOADS_DIR / 'missing-deepeval-export.json'\n",
    "\n",
    "if not DEEPEVAL_EXPORT_PATH.exists():\n",
    "    raise FileNotFoundError(f'Update DEEPEVAL_EXPORT_PATH first: {DEEPEVAL_EXPORT_PATH}')\n",
    "\n",
    "export_payload = json.loads(DEEPEVAL_EXPORT_PATH.read_text())\n",
    "test_specs = export_payload['dataset']['test_cases']\n",
    "len(test_specs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Demo: Call your app to get actual outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_API_URL = 'http://localhost:8000/chat'  # change to your app endpoint\n",
    "\n",
    "def call_app(question: str):\n",
    "    resp = requests.post(APP_API_URL, json={'question': question}, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    data = resp.json()\n",
    "    if isinstance(data, dict) and 'answer' in data:\n",
    "        return data.get('answer', ''), data.get('contexts', []) or []\n",
    "\n",
    "    return str(data), []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = []\n",
    "\n",
    "for spec in test_specs:\n",
    "    actual_output, retrieved_context = call_app(spec['input'])\n",
    "\n",
    "    case = LLMTestCase(\n",
    "        input=spec['input'],\n",
    "        actual_output=actual_output,\n",
    "        expected_output=spec.get('expected_output', ''),\n",
    "        retrieval_context=retrieved_context if isinstance(retrieved_context, list) else [str(retrieved_context)],\n",
    "    )\n",
    "    cases.append(case)\n",
    "\n",
    "len(cases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure metrics and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    AnswerRelevancyMetric(threshold=0.7),\n",
    "    HallucinationMetric(threshold=0.5),\n",
    "]\n",
    "\n",
    "evaluation_result = evaluate(test_cases=cases, metrics=metrics)\n",
    "evaluation_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Save a simple report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = PROJECT_ROOT / 'outputs'\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "report_path = OUT_DIR / 'deepeval_summary.txt'\n",
    "report_path.write_text(str(evaluation_result))\n",
    "report_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
