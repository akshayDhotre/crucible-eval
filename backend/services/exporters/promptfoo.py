from __future__ import annotations

import os
from typing import Any

import yaml

PROVIDER_TARGETS = {
    "openai": "openai:gpt-4o",
    "anthropic": "anthropic:messages:claude-sonnet-4-6",
    "google": "google:gemini-2.0-flash",
    "ollama": "ollama:chat:llama3.1",
    "lmstudio": "openai:chat:local-model",
}
PROVIDER_PREFIX = {
    "openai": "openai",
    "anthropic": "anthropic:messages",
    "google": "google",
    "ollama": "ollama:chat",
    "lmstudio": "openai:chat",
}


def build_promptfoo_config(suite: dict[str, Any], provider: str) -> str:
    tests = []
    for case in suite.get("testCases", []):
        assertions = []
        if case.get("expectedOutput"):
            assertions.append({"type": "contains", "value": case["expectedOutput"]})
        for criterion in case.get("evalCriteria", []):
            assertions.append({"type": "llm-rubric", "value": criterion})

        tests.append(
            {
                "description": f"{case.get('id', 'case')} [{case.get('category', 'unknown')}]",
                "vars": {"input": case.get("input", "")},
                "assert": assertions or [{"type": "is-json"}],
            }
        )

    model_override = os.getenv("DEFAULT_MODEL_NAME", "").strip()
    provider_target = PROVIDER_TARGETS.get(provider, "openai:gpt-4o")
    if model_override:
        provider_target = f"{PROVIDER_PREFIX.get(provider, 'openai')}:{model_override}"

    payload = {
        "description": "Generated by Crucible Eval",
        "prompts": ["{{input}}"],
        "providers": [provider_target],
        "defaultTest": {"options": {"transformVars": "return vars;"}},
        "tests": tests,
    }
    return yaml.safe_dump(payload, sort_keys=False)
